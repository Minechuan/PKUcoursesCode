{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5e0e20",
   "metadata": {},
   "source": [
    "# Text to text DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48178744",
   "metadata": {},
   "source": [
    "在选择模型的数据集的过程中：首先尝试了 encoder-decoder 架构的 t5-small ，但是模型的生成效果不好。于是改用作业中 work 的 Qwen2.5-0.5B-Instruct。尝试了 Dahoas/rm-static，Anthropic/hh-rlhf 数据集，但是他们都是多轮对话，同时 string 长度较大，针对 Human 的提问手动设置 label 并不容易。因此我们选择：**Intel/orca_dpo_pairs** 数据集。其中有\n",
    "\n",
    "此外，我们使用了 **Qwen-2.5-0.5B-Instruct** 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692b4ff",
   "metadata": {},
   "source": [
    "由于使用 NPU，在命令行中输入 ``source /usr/local/Ascend/ascend-toolkit/set_env.sh``\n",
    "运行 python 文件可行。但是使用 ipynb 文件就遇到了许多困难，环境变量不会“回传”给 Python 内核。\n",
    "所以对于我使用的的服务器（910B）：尝试 **启动前先 source，再 jupyter notebook** 。\n",
    "步骤如下：\n",
    "```bash\n",
    "conda activate align-anything\n",
    "source /usr/local/Ascend/ascend-toolkit/set_env.sh\n",
    "jupyter notebook --allow-root\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a95b1",
   "metadata": {},
   "source": [
    "### 准备工作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa4ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch_npu/__init__.py:248: UserWarning: On the interactive interface, the value of TASK_QUEUE_ENABLE is set to 0 by default.                      Do not set it to 1 to prevent some unknown errors\n",
      "  warnings.warn(\"On the interactive interface, the value of TASK_QUEUE_ENABLE is set to 0 by default. \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my device is: npu:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 设置 Hugging Face 镜像\n",
    "os.environ[\"HF_HUB_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# os.environ[\"ASCEND_HOME_PATH\"]=\"/usr/local/Ascend\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=torch.device(\"npu:0\")\n",
    "print(\"my device is:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6359376",
   "metadata": {},
   "source": [
    "### 设置模型、数据集和超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9fcc18",
   "metadata": {},
   "source": [
    "在实现的过程中训练很容易不稳定，导致模型输入无关的 token 或 **不输出**，经过一系列的修改和调参，得到了以下的参数组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d2462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "ref_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # 参考策略\n",
    "# dataset_name=\"Dahoas/rm-static\"\n",
    "dataset_name=\"Intel/orca_dpo_pairs\" # 采用单轮对话\n",
    "save_dir = \"./qwen-ft\"\n",
    "max_length=512\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 8e-5\n",
    "num_epochs = 1\n",
    "beta = 0.1  # DPO 温度参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117aa016",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11998261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-09 23:15:12,003] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to npu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miniconda3/envs/align-anything/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device) # 显存占用 60 G\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(ref_model_name).eval().to(device)\n",
    "model.config.sliding_window = None  # 显式禁用\n",
    "ref_model.config.sliding_window = None  # 显式禁用\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750675f",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6281a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, tokenizer, split=\"train\", max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        system_msg = sample[\"system\"]\n",
    "        user_msg = sample[\"question\"]\n",
    "        better_response = sample[\"chosen\"]\n",
    "        worse_response = sample[\"rejected\"]\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg}\n",
    "        ]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        text_tokens = self.tokenizer(text, add_special_tokens=False)\n",
    "        text_len = len(text_tokens[\"input_ids\"])\n",
    "\n",
    "        # 拼接 better\n",
    "        better_full = text + better_response\n",
    "        better_enc = self.tokenizer(\n",
    "            better_full,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # 使用了 apply_chat_template，它会自动在结尾加上 <|assistant|>\\n\n",
    "        # 拼接 worse\n",
    "        worse_full = text + worse_response\n",
    "        worse_enc = self.tokenizer(\n",
    "            worse_full,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids_better\": better_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_better\": better_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_worse\": worse_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_worse\": worse_enc[\"attention_mask\"].squeeze(0),\n",
    "            # \"text_ids\": text_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"text_len\": text_len  \n",
    "        }\n",
    "train_dataset = PreferenceDataset(tokenizer,split=\"train\",max_length=max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f943d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.', 'question': 'Q: Answer the following question given this paragraph:   The kidneys also secrete hormones that help maintain homeostasis. For example, they produce a hormone that stimulates bone marrow to produce red blood cells when more are needed. They also secrete a hormone that regulates blood pressure and keeps it in a normal range.   Q: What organs secrete hormones that help maintain homeostasis?   A:\\nThe answer is:', 'chosen': 'The kidneys are the organs that secrete hormones to help maintain homeostasis. They produce a hormone that stimulates bone marrow to produce red blood cells when needed, and they also secrete a hormone that regulates blood pressure, keeping it within a normal range.', 'rejected': ' Certainly! Based on the provided paragraph, the organs that secrete hormones to help maintain homeostasis are the kidneys. The kidneys produce two hormones that help regulate various physiological processes in the body:\\n\\n1. Erythropoietin (EPO): This hormone stimulates the bone marrow to produce red blood cells when there is a decrease in the number of red blood cells in the body. This helps to maintain normal red blood cell levels and prevent anemia.\\n2. Renin: This hormone regulates blood pressure by controlling the amount of fluid in the body and the diameter of blood vessels. When blood pressure is low, the kidneys produce more renin, which causes the blood vessels to constrict and retain more fluid. This helps to increase blood pressure back to normal levels.\\n\\nSo, to summarize, the organs that secrete hormones to help maintain homeostasis are the kidneys, specifically the nephrons within the kidneys. These hormones play a crucial role in regulating various physiological processes, such as red blood cell production and blood pressure, to maintain homeostasis in the body.'}\n",
      "the length of the train dataset is: 12859\n"
     ]
    }
   ],
   "source": [
    "'''data set'''\n",
    "\n",
    "print(train_dataset.dataset[10])\n",
    "print(\"the length of the train dataset is:\",len(train_dataset.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169654af",
   "metadata": {},
   "source": [
    "### 设置 Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc78d7",
   "metadata": {},
   "source": [
    "### DPO loss\n",
    "$$\n",
    "\\mathcal{L}_{DPO}(\\pi_{\\theta};\\pi_{ref})=-\\mathbb{E}_{(x,y_w,y_l)\\sim D}\\left[\\log \\sigma(\\beta\\log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)}-\\beta\\log \\frac{\\pi_{\\theta}(y_l | x)}{\\pi_{ref}(y_l | x)})\\right]\n",
    "$$\n",
    "**最初的尝试**\n",
    "\n",
    "首先计算模型输出的 Negative Log-Likelihood：\n",
    "$$\n",
    "out\\_worse.loss=-\\dfrac{1}{N}\\sum\\limits_{i=1}^N\\log P_{\\theta}(y_i|x_i)\n",
    "$$\n",
    "然后在求 $\\log \\pi_{\\theta}(y_w | x)$ 时使用： $\\log\\pi_{\\theta}(y_l | x)= -out\\_worse\\_theta.loss$\n",
    "\n",
    "但是应该采用整个序列预测 token 的对数概率之和，**采用**：\n",
    "$$\n",
    "\\sum\\limits_{i=k+1}^N\\log P_{\\theta}(y_i|x_{t<i})\n",
    "$$\n",
    "其中 k 是不需要计算 loss 的 prompt 输入 token 个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(logp_theta_w, logp_ref_w, logp_theta_l, logp_ref_l, beta):\n",
    "    \n",
    "    diff = beta * ((logp_theta_w - logp_ref_w) - (logp_theta_l - logp_ref_l))\n",
    "    # 负对数 sigmoid\n",
    "    loss = -F.logsigmoid(diff).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c6258",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae5fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1608/1608 [28:43<00:00,  1.07s/it, loss=0.0014, avg_loss=0.1566]\n"
     ]
    }
   ],
   "source": [
    "def get_log_probs(model, input_ids, attention_mask, labels, is_ref_model=False):\n",
    "    # 如果是参考模型，强制禁用梯度\n",
    "    if is_ref_model:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    else:\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    selected_log_probs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "    mask = (labels != -100)\n",
    "    return (selected_log_probs * mask).sum(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_batches = len(train_dataloader)\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        \n",
    "\n",
    "        better_ids   = batch[\"input_ids_better\"].to(device)      # [B, Lp]\n",
    "        better_att  = batch[\"attention_mask_better\"].to(device)\n",
    "        worse_ids    = batch[\"input_ids_worse\"].to(device)      # [B, Lc]\n",
    "        worse_att   = batch[\"attention_mask_worse\"].to(device)\n",
    "        # prompt_ids   = batch[\"text_ids\"].to(device)    # [B, Lr]\n",
    "        prompt_len   = batch[\"text_len\"].to(device)\n",
    "\n",
    "        # make labels\n",
    "        B, L = better_ids.shape\n",
    "        w_labels = better_ids.clone()\n",
    "        token_pos = torch.arange(L, device=device).unsqueeze(0).expand(B, L)\n",
    "        w_labels[token_pos < prompt_len.unsqueeze(1)] = -100\n",
    "        l_labels = worse_ids.clone()\n",
    "        l_labels[token_pos < prompt_len.unsqueeze(1)] = -100\n",
    "\n",
    "        # 计算策略模型的 log-probs（保留梯度）\n",
    "        logp_w_theta = get_log_probs(model, better_ids, better_att, w_labels, is_ref_model=False)\n",
    "        logp_l_theta = get_log_probs(model, worse_ids, worse_att, l_labels, is_ref_model=False)\n",
    "\n",
    "        # 计算参考模型的 log-probs（禁用梯度）\n",
    "        logp_w_ref = get_log_probs(ref_model, better_ids, better_att, w_labels, is_ref_model=True)\n",
    "        logp_l_ref = get_log_probs(ref_model, worse_ids, worse_att, l_labels, is_ref_model=True)\n",
    "\n",
    "        # ===== DPO loss + backward =====\n",
    "\n",
    "        loss = dpo_loss(logp_w_theta, logp_w_ref, logp_l_theta, logp_l_ref, beta)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "\n",
    "        # 更新进度条显示\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{avg_loss:.4f}'\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b09daf",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c30abc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qwen-ft/tokenizer_config.json',\n",
       " './qwen-ft/special_tokens_map.json',\n",
       " './qwen-ft/vocab.json',\n",
       " './qwen-ft/merges.txt',\n",
       " './qwen-ft/added_tokens.json',\n",
       " './qwen-ft/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859f817",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d27611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Output ===\n",
      "system\n",
      "You are an AI assistant that helps people find information.\n",
      "user\n",
      "Where would the best place to drive over the speed limit be?\n",
      "assistant\n",
      "\n",
      "\n",
      "=== Reference Model Output ===\n",
      "system\n",
      "You are an AI assistant that helps people find information.\n",
      "user\n",
      "Where would the best place to drive over the speed limit be?\n",
      "assistant\n",
      "The best place to drive over the speed limit will depend on various factors, including the specific location and circumstances of the driver. However, generally speaking:\n",
      "\n",
      "1. **Highway**: On highways, it's often safer to drive at a higher speed limit because there is less traffic and more room for maneuvering.\n",
      "\n",
      "2. **Urban Areas**: In urban areas with limited space, driving at high speeds can lead to accidents due to reduced visibility and increased risk of collisions.\n",
      "\n",
      "3. **School Zones**: In\n"
     ]
    }
   ],
   "source": [
    "system_msg = \"You are an AI assistant that helps people find information.\"\n",
    "user_msg   = \"Where would the best place to drive over the speed limit be?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_msg},\n",
    "    {\"role\": \"user\", \"content\": user_msg}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=100, do_sample=True)\n",
    "    ref_output_ids = ref_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "ref_output_text = tokenizer.decode(ref_output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=== Model Output ===\")\n",
    "print(output_text)\n",
    "print(\"\\n=== Reference Model Output ===\")\n",
    "print(ref_output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "align-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
