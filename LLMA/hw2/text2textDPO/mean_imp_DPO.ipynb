{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5e0e20",
   "metadata": {},
   "source": [
    "# Text to text DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48178744",
   "metadata": {},
   "source": [
    "在选择模型的数据集的过程中：首先尝试了 encoder-decoder 架构的 t5-small ，但是模型的生成效果不好。于是改用作业中 work 的 Qwen2.5-0.5B-Instruct。尝试了 Dahoas/rm-static，Anthropic/hh-rlhf 数据集，但是他们都是多轮对话，同时 string 长度较大，针对 Human 的提问手动设置 label 并不容易。因此我们选择：**Intel/orca_dpo_pairs** 数据集。其中有\n",
    "\n",
    "此外，我们使用了 **Qwen-2.5-0.5B-Instruct** 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a95b1",
   "metadata": {},
   "source": [
    "### 准备工作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa4ce5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\npartially initialized module 'torch_npu' has no attribute 'npu' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/import_utils.py:1967\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1966\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/generation/utils.py:42\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_deepspeed_zero3_enabled\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfsdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fsdp_managed_module\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/integrations/deepspeed.py:50\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_deepspeed_available():\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfDeepSpeedConfig \u001b[38;5;28;01mas\u001b[39;00m DeepSpeedConfig\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# Inherits from a dummy `object` if accelerate is not available, so that python succeeds to import this file.\u001b[39;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# Deepspeed glue code will never inherit this dummy object as it checks if accelerate is available.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/accelerate/__init__.py:16\u001b[39m\n\u001b[32m     14\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m1.6.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maccelerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     cpu_offload,\n\u001b[32m     19\u001b[39m     cpu_offload_with_hook,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     load_checkpoint_and_dispatch,\n\u001b[32m     25\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/accelerate/accelerator.py:38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_torchao_available\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpointing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/accelerate/checkpointing.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     MODEL_NAME,\n\u001b[32m     25\u001b[39m     OPTIMIZER_NAME,\n\u001b[32m     26\u001b[39m     RNG_STATE_NAME,\n\u001b[32m     27\u001b[39m     SAFE_MODEL_NAME,\n\u001b[32m     28\u001b[39m     SAFE_WEIGHTS_NAME,\n\u001b[32m     29\u001b[39m     SAMPLER_NAME,\n\u001b[32m     30\u001b[39m     SCALER_NAME,\n\u001b[32m     31\u001b[39m     SCHEDULER_NAME,\n\u001b[32m     32\u001b[39m     WEIGHTS_NAME,\n\u001b[32m     33\u001b[39m     get_pretty_name,\n\u001b[32m     34\u001b[39m     is_cuda_available,\n\u001b[32m     35\u001b[39m     is_hpu_available,\n\u001b[32m     36\u001b[39m     is_mlu_available,\n\u001b[32m     37\u001b[39m     is_musa_available,\n\u001b[32m     38\u001b[39m     is_sdaa_available,\n\u001b[32m     39\u001b[39m     is_torch_xla_available,\n\u001b[32m     40\u001b[39m     is_xpu_available,\n\u001b[32m     41\u001b[39m     load,\n\u001b[32m     42\u001b[39m     save,\n\u001b[32m     43\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/accelerate/utils/__init__.py:140\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     84\u001b[39m     deepspeed_required,\n\u001b[32m     85\u001b[39m     get_ccl_version,\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m     torchao_required,\n\u001b[32m    139\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    141\u001b[39m     align_module_device,\n\u001b[32m    142\u001b[39m     calculate_maximum_sizes,\n\u001b[32m    143\u001b[39m     check_device_map,\n\u001b[32m    144\u001b[39m     check_tied_parameters_in_config,\n\u001b[32m    145\u001b[39m     check_tied_parameters_on_same_device,\n\u001b[32m    146\u001b[39m     compute_module_sizes,\n\u001b[32m    147\u001b[39m     convert_file_size_to_int,\n\u001b[32m    148\u001b[39m     dtype_byte_size,\n\u001b[32m    149\u001b[39m     find_tied_parameters,\n\u001b[32m    150\u001b[39m     get_balanced_memory,\n\u001b[32m    151\u001b[39m     get_grad_scaler,\n\u001b[32m    152\u001b[39m     get_max_layer_size,\n\u001b[32m    153\u001b[39m     get_max_memory,\n\u001b[32m    154\u001b[39m     get_mixed_precision_context_manager,\n\u001b[32m    155\u001b[39m     has_offloaded_params,\n\u001b[32m    156\u001b[39m     id_tensor_storage,\n\u001b[32m    157\u001b[39m     infer_auto_device_map,\n\u001b[32m    158\u001b[39m     is_peft_model,\n\u001b[32m    159\u001b[39m     load_checkpoint_in_model,\n\u001b[32m    160\u001b[39m     load_offloaded_weights,\n\u001b[32m    161\u001b[39m     load_state_dict,\n\u001b[32m    162\u001b[39m     named_module_tensors,\n\u001b[32m    163\u001b[39m     retie_parameters,\n\u001b[32m    164\u001b[39m     set_module_tensor_to_device,\n\u001b[32m    165\u001b[39m )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01moffload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    167\u001b[39m     OffloadedWeightsLoader,\n\u001b[32m    168\u001b[39m     PrefixedDataset,\n\u001b[32m   (...)\u001b[39m\u001b[32m    173\u001b[39m     save_offload_index,\n\u001b[32m    174\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/accelerate/utils/modeling.py:32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AcceleratorState\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAFE_WEIGHTS_NAME, WEIGHTS_NAME\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/accelerate/state.py:72\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_musa\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_npu_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_device\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_npu\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/accelerate/utils/imports.py:404\u001b[39m, in \u001b[36mis_npu_available\u001b[39m\u001b[34m(check_device)\u001b[39m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_npu\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_device:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch_npu/__init__.py:167\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# rename device name to 'npu' and register funcs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m torch._register_device_module(\u001b[33m'\u001b[39m\u001b[33mnpu\u001b[39m\u001b[33m'\u001b[39m, \u001b[43mtorch_npu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnpu\u001b[49m)\n\u001b[32m    168\u001b[39m unsupported_dtype = [torch.quint8, torch.quint4x2, torch.quint2x4, torch.qint32, torch.qint8]\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch_npu' has no attribute 'npu' (most likely due to a circular import)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/import_utils.py:1967\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1966\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:21\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     _BaseAutoBackboneClass,\n\u001b[32m     23\u001b[39m     _BaseAutoModelClass,\n\u001b[32m     24\u001b[39m     _LazyAutoMapping,\n\u001b[32m     25\u001b[39m     auto_class_update,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_MAPPING_NAMES\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:40\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[32m     43\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/import_utils.py:1955\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1956\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/import_utils.py:1969\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1970\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1971\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1972\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\npartially initialized module 'torch_npu' has no attribute 'npu' (most likely due to a circular import)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/import_utils.py:1956\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   1955\u001b[39m     module = \u001b[38;5;28mself\u001b[39m._get_module(\u001b[38;5;28mself\u001b[39m._class_to_module[name])\n\u001b[32m-> \u001b[39m\u001b[32m1956\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1957\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m   1958\u001b[39m     value = \u001b[38;5;28mself\u001b[39m._get_module(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/import_utils.py:1955\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1953\u001b[39m     value = Placeholder\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1956\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1957\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/import_utils.py:1969\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1970\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1971\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1972\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\npartially initialized module 'torch_npu' has no attribute 'npu' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 使用 hf-mirror 下载模型\n",
    "os.environ[\"HF_HUB_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "wandb.login(key=\"My key\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=torch.device(\"npu:0\")\n",
    "print(\"my device is:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6359376",
   "metadata": {},
   "source": [
    "### 设置模型、数据集和超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d2462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "ref_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # 参考策略\n",
    "# dataset_name=\"Dahoas/rm-static\"\n",
    "dataset_name=\"Intel/orca_dpo_pairs\" # 采用单轮对话\n",
    "save_dir = \"./qwen-ft\"\n",
    "max_length=512\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 2\n",
    "beta = 0.01  # DPO 温度参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117aa016",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11998261",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Initialize:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:247 NPU function error: at_npu::native::AclSetCompileopt(aclCompileOpt::ACL_PRECISION_MODE, precision_mode), error code is 500001\n[ERROR] 2025-05-09-10:44:20 (PID:2869731, Device:0, RankID:-1) ERR00100 PTA call acl api failed\n[Error]: The internal ACL of the system is incorrect.\n        Rectify the fault based on the error information in the ascend log.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 显存占用 50 G\u001b[39;00m\n\u001b[32m      3\u001b[39m ref_model = AutoModelForCausalLM.from_pretrained(ref_model_name).eval().to(device)\n\u001b[32m      4\u001b[39m model.config.sliding_window = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# 显式禁用\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/modeling_utils.py:3698\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3693\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3694\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3697\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3698\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch_npu/utils/_module.py:70\u001b[39m, in \u001b[36mto\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch_npu.npu.is_available():\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcast_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(t):\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() == \u001b[32m4\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch_npu/utils/_module.py:133\u001b[39m, in \u001b[36mcast_weight\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    132\u001b[39m current_class = \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43m_format_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children:\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch_npu/utils/_module.py:96\u001b[39m, in \u001b[36mcast_weight.<locals>._format_cast\u001b[39m\u001b[34m(module, class_name)\u001b[39m\n\u001b[32m     93\u001b[39m     module.v_proj_weight.data = module.v_proj_weight.data.to(device)\n\u001b[32m     94\u001b[39m     module.v_proj_weight.data = torch_npu.npu_format_cast(module.v_proj_weight.data, \u001b[32m29\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_jit_compile_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(class_name, (torch.nn.BatchNorm2d, torch.nn.BatchNorm1d)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch_npu/npu/npu_config.py:132\u001b[39m, in \u001b[36mis_jit_compile_false\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_jit_compile_false\u001b[39m() -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[43mtorch_npu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_npu._C._npu_is_jit_compile_false()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch_npu/npu/__init__.py:215\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_internal_in_bad_fork:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    212\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize NPU in forked subprocess. To use NPU with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[43mtorch_npu\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_npu_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m _original_pid = os.getpid()\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Initialize:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:247 NPU function error: at_npu::native::AclSetCompileopt(aclCompileOpt::ACL_PRECISION_MODE, precision_mode), error code is 500001\n[ERROR] 2025-05-09-10:44:20 (PID:2869731, Device:0, RankID:-1) ERR00100 PTA call acl api failed\n[Error]: The internal ACL of the system is incorrect.\n        Rectify the fault based on the error information in the ascend log.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device) # 显存占用 50 G\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(ref_model_name).eval().to(device)\n",
    "model.config.sliding_window = None  # 显式禁用\n",
    "ref_model.config.sliding_window = None  # 显式禁用\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750675f",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, tokenizer, split=\"train\", max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        system_msg = sample[\"system\"]\n",
    "        user_msg = sample[\"question\"]\n",
    "        better_response = sample[\"chosen\"]\n",
    "        worse_response = sample[\"rejected\"]\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg}\n",
    "        ]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        text_tokens = self.tokenizer(text, add_special_tokens=False)\n",
    "        text_len = len(text_tokens[\"input_ids\"])\n",
    "\n",
    "        # 拼接 better\n",
    "        better_full = text + better_response\n",
    "        better_enc = self.tokenizer(\n",
    "            better_full,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # 使用了 apply_chat_template，它会自动在结尾加上 <|assistant|>\\n\n",
    "        # 拼接 worse\n",
    "        worse_full = text + worse_response\n",
    "        worse_enc = self.tokenizer(\n",
    "            worse_full,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids_better\": better_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_better\": better_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_worse\": worse_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_worse\": worse_enc[\"attention_mask\"].squeeze(0),\n",
    "            # \"text_ids\": text_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"text_len\": text_len  \n",
    "        }\n",
    "train_dataset = PreferenceDataset(tokenizer,split=\"train\",max_length=max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f943d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:', 'response': ' Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it’s lint-free.', 'chosen': ' Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it’s lint-free.', 'rejected': ' Yes, you can spray it directly onto the cloth.'}\n",
      "the length of the train dataset and test dataset is: 76256 5103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dpo-t5-small</strong> at: <a href='https://wandb.ai/minechuan-peking-university/t5-dpo-training/runs/eqw124pn' target=\"_blank\">https://wandb.ai/minechuan-peking-university/t5-dpo-training/runs/eqw124pn</a><br> View project at: <a href='https://wandb.ai/minechuan-peking-university/t5-dpo-training' target=\"_blank\">https://wandb.ai/minechuan-peking-university/t5-dpo-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250508_122014-eqw124pn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/my_used/wandb/run-20250508_122247-5gipu81u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/minechuan-peking-university/t5-dpo-training/runs/5gipu81u' target=\"_blank\">dpo-t5-small</a></strong> to <a href='https://wandb.ai/minechuan-peking-university/t5-dpo-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/minechuan-peking-university/t5-dpo-training' target=\"_blank\">https://wandb.ai/minechuan-peking-university/t5-dpo-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/minechuan-peking-university/t5-dpo-training/runs/5gipu81u' target=\"_blank\">https://wandb.ai/minechuan-peking-university/t5-dpo-training/runs/5gipu81u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/minechuan-peking-university/t5-dpo-training/runs/5gipu81u?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0xfffefd62e590>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "'''data set'''\n",
    "\n",
    "print(train_dataset.dataset[0])\n",
    "print(\"the length of the train dataset is:\",len(train_dataset.dataset))\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"dpo-training\",\n",
    "    name=\"run_exp\",\n",
    "    config={\n",
    "        \"model_name\": model_name,\n",
    "        \"ref_model_name\": ref_model_name,\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"beta\": beta,\n",
    "        \"max_length\": max_length, # in dataset prompt 256+ response 256\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169654af",
   "metadata": {},
   "source": [
    "### 设置 Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc78d7",
   "metadata": {},
   "source": [
    "### DPO loss\n",
    "$$\n",
    "\\mathcal{L}_{DPO}(\\pi_{\\theta};\\pi_{ref})=-\\mathbb{E}_{(x,y_w,y_l)\\sim D}\\left[\\log \\sigma(\\beta\\log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)}-\\beta\\log \\frac{\\pi_{\\theta}(y_l | x)}{\\pi_{ref}(y_l | x)})\\right]\n",
    "$$\n",
    "经过模型的输出：\n",
    "$$\n",
    "out\\_worse.loss=\\dfrac{1}{N}\\sum\\limits_{i=1}^N-\\log P_{model}(y_i|x_i)\n",
    "$$\n",
    "例如我们在求 $\\log \\pi_{\\theta}(y_w | x)$ 时使用：\n",
    "$$\n",
    "-out\\_worse\\_theta.loss\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(logp_theta_w, logp_ref_w, logp_theta_l, logp_ref_l, beta):\n",
    "    \n",
    "    diff = beta * ((logp_theta_w - logp_ref_w) - (logp_theta_l - logp_ref_l))\n",
    "    # 负对数 sigmoid\n",
    "    loss = -F.logsigmoid(diff).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c6258",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae5fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.2117\n",
      "Epoch 0 - Loss: 0.8887\n",
      "Epoch 0 - Loss: 5.6476\n",
      "Epoch 0 - Loss: 0.1030\n",
      "Epoch 0 - Loss: 2.2070\n",
      "Epoch 0 - Loss: 2.2786\n",
      "Epoch 0 - Loss: 0.0949\n",
      "Epoch 0 - Loss: 1.1465\n",
      "Epoch 0 - Loss: 0.4953\n",
      "Epoch 0 - Loss: 8.6514\n",
      "Epoch 0 - Loss: 0.0001\n",
      "Epoch 0 - Loss: 10.8875\n",
      "Epoch 0 - Loss: 18.6852\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m dpo_loss(logp_theta_w, logp_ref_w, logp_theta_l, logp_ref_l, beta)\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\project\\python3.10.11\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\project\\python3.10.11\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\project\\python3.10.11\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_batches = len(train_dataloader)\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        \n",
    "\n",
    "        better_ids   = batch[\"input_ids_better\"].to(device)      # [B, Lp]\n",
    "        better_att  = batch[\"attention_mask_better\"].to(device)\n",
    "        worse_ids    = batch[\"input_ids_worse\"].to(device)      # [B, Lc]\n",
    "        worse_att   = batch[\"attention_mask_worse\"].to(device)\n",
    "        # prompt_ids   = batch[\"text_ids\"].to(device)    # [B, Lr]\n",
    "        prompt_len   = batch[\"text_len\"].to(device)\n",
    "\n",
    "        # make labels\n",
    "        B, L = better_ids.shape\n",
    "        w_labels = better_ids.clone()\n",
    "        token_pos = torch.arange(L, device=device).unsqueeze(0).expand(B, L)\n",
    "        w_labels[token_pos < prompt_len.unsqueeze(1)] = -100\n",
    "        l_labels = worse_ids.clone()\n",
    "        l_labels[token_pos < prompt_len.unsqueeze(1)] = -100\n",
    "\n",
    "        # === Get better log-probs ===\n",
    "        out_better = model(\n",
    "            input_ids=better_ids,\n",
    "            attention_mask=better_att,\n",
    "            labels= w_labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        # token_w = (token_pos >= prompt_len.unsqueeze(1)).sum(dim=1).float().clamp(min=1.)\n",
    "        # average loss for every token\n",
    "        logp_w_theta = -out_better.loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_better_ref = ref_model(\n",
    "                input_ids=better_ids,\n",
    "                attention_mask=better_att,\n",
    "                labels= w_labels,\n",
    "                return_dict=True\n",
    "            )\n",
    "            logp_w_ref = -out_better_ref.loss\n",
    "\n",
    "        # === Get better log-probs ===\n",
    "        out_worse = model(\n",
    "            input_ids=worse_ids,\n",
    "            attention_mask=worse_att,\n",
    "            labels= l_labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        # token_l = (token_pos >= prompt_len.unsqueeze(1)).sum(dim=1).float().clamp(min=1.)\n",
    "        logp_l_theta = -out_worse.loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_worse_ref = ref_model(\n",
    "                input_ids=worse_ids,\n",
    "                attention_mask=worse_att,\n",
    "                labels= l_labels,\n",
    "                return_dict=True\n",
    "            )\n",
    "            logp_l_ref = -out_worse_ref.loss\n",
    "\n",
    "        # ===== DPO loss + backward =====\n",
    "\n",
    "        loss = dpo_loss(logp_w_theta, logp_w_ref, logp_l_theta, logp_l_ref, beta)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "\n",
    "        # 更新进度条显示\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{avg_loss:.4f}'\n",
    "        })\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            wandb.log({\"loss\": loss.item(), \"avg_loss\": avg_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b09daf",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30abc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "wandb.save(os.path.join(save_dir, \"pytorch_model.bin\"))\n",
    "wandb.finish()\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859f817",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = \"You are an AI assistant that helps people find information.\"\n",
    "user_msg   = \"Where would the best place to drive over the speed limit be?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_msg},\n",
    "    {\"role\": \"user\", \"content\": user_msg}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    ref_output_ids = ref_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "ref_output_text = tokenizer.decode(ref_output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=== Model Output ===\")\n",
    "print(output_text)\n",
    "print(\"\\n=== Reference Model Output ===\")\n",
    "print(ref_output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "align-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
