{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5e0e20",
   "metadata": {},
   "source": [
    "# Text to text DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48178744",
   "metadata": {},
   "source": [
    "在选择模型的数据集的过程中：首先尝试了 encoder-decoder 架构的 t5-small ，但是模型的生成效果不好。于是改用作业中 work 的 Qwen2.5-0.5B-Instruct。尝试了 Dahoas/rm-static，Anthropic/hh-rlhf 数据集，但是他们都是多轮对话，同时 string 长度较大，针对 Human 的提问手动设置 label 并不容易。因此我们选择：**Intel/orca_dpo_pairs** 数据集。其中有\n",
    "\n",
    "此外，我们使用了 **Qwen-2.5-0.5B-Instruct** 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692b4ff",
   "metadata": {},
   "source": [
    "由于使用 NPU，在命令行中输入 ``source /usr/local/Ascend/ascend-toolkit/set_env.sh``\n",
    "运行 python 文件可行。但是使用 ipynb 文件就遇到了许多困难，环境变量不会“回传”给 Python 内核。\n",
    "所以对于我使用的的服务器（910B）：尝试 **启动前先 source，再 jupyter notebook** 。\n",
    "步骤如下：\n",
    "```bash\n",
    "conda activate align-anything\n",
    "source /usr/local/Ascend/ascend-toolkit/set_env.sh\n",
    "jupyter notebook --allow-root\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a95b1",
   "metadata": {},
   "source": [
    "### 准备工作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa4ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch_npu/__init__.py:248: UserWarning: On the interactive interface, the value of TASK_QUEUE_ENABLE is set to 0 by default.                      Do not set it to 1 to prevent some unknown errors\n",
      "  warnings.warn(\"On the interactive interface, the value of TASK_QUEUE_ENABLE is set to 0 by default. \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my device is: npu:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 设置 Hugging Face 镜像\n",
    "os.environ[\"HF_HUB_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# os.environ[\"ASCEND_HOME_PATH\"]=\"/usr/local/Ascend\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=torch.device(\"npu:0\")\n",
    "print(\"my device is:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6359376",
   "metadata": {},
   "source": [
    "### 设置模型、数据集和超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d2462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "ref_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # 参考策略\n",
    "# dataset_name=\"Dahoas/rm-static\"\n",
    "dataset_name=\"Intel/orca_dpo_pairs\" # 采用单轮对话\n",
    "save_dir = \"./dpo-FT\"\n",
    "\n",
    "max_length=512\n",
    "\n",
    "# batch size is 64\n",
    "grad_accum_steps = 8\n",
    "batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 1\n",
    "beta = 0.1  # DPO 温度参数\n",
    "\n",
    "subset_size = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117aa016",
   "metadata": {},
   "source": [
    "### 加载模型\n",
    "\n",
    "设置 tokenizer ，同时冻结 ref model 的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11998261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-23 10:52:47,568] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to npu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miniconda3/envs/align-anything/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 添加\n",
    "tokenizer.padding_side = \"left\"            # 左填充（适合decoder）\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device) # 显存占用 60 G\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(ref_model_name).eval().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750675f",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6281a695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is: 512. Filtered dataset: 3000/12859 samples remaining\n"
     ]
    }
   ],
   "source": [
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, tokenizer, split=\"train\", max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.dataset = load_dataset(dataset_name, split=split)\n",
    "        \n",
    "        # 预处理：过滤掉text_len > max_length的样本\n",
    "        self.valid_indices = []\n",
    "        for idx in range(len(self.dataset)):\n",
    "            sample = self.dataset[idx]\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": sample[\"system\"]},\n",
    "                {\"role\": \"user\", \"content\": sample[\"question\"]}\n",
    "            ]\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            text_tokens = self.tokenizer(text, add_special_tokens=False)\n",
    "            if len(text_tokens[\"input_ids\"]) <= max_length/2:\n",
    "                self.valid_indices.append(idx)\n",
    "        \n",
    "        \n",
    "        if subset_size is not None:\n",
    "                self.valid_indices = self.valid_indices[:subset_size]  # 只保留前 subset_size 个样本\n",
    "            \n",
    "                \n",
    "        print(f\"Max length is: {max_length}. Filtered dataset: {len(self.valid_indices)}/{len(self.dataset)} samples remaining\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)  # 返回过滤后的长度\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 使用过滤后的索引获取原始数据\n",
    "        original_idx = self.valid_indices[idx]\n",
    "        sample = self.dataset[original_idx]\n",
    "        \n",
    "        system_msg = sample[\"system\"]\n",
    "        user_msg = sample[\"question\"]\n",
    "        better_response = sample[\"chosen\"]\n",
    "        worse_response = sample[\"rejected\"]\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg}\n",
    "        ]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # 动态计算prompt长度（已确保<=max_length）\n",
    "        text_tokens = self.tokenizer(text, add_special_tokens=False)\n",
    "        prompt_len = len(text_tokens[\"input_ids\"])\n",
    "\n",
    "        # 处理better和worse样本\n",
    "        better_full = text + better_response\n",
    "        better_enc = self.tokenizer(\n",
    "            better_full,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        worse_full = text + worse_response\n",
    "        worse_enc = self.tokenizer(\n",
    "            worse_full,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids_better\": better_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_better\": better_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_worse\": worse_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_worse\": worse_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"text_len\": prompt_len\n",
    "        }\n",
    "\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "# \n",
    "train_dataset = PreferenceDataset(tokenizer,split=\"train\",max_length=max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f943d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.', 'question': 'Q: Answer the following question given this paragraph:   The kidneys also secrete hormones that help maintain homeostasis. For example, they produce a hormone that stimulates bone marrow to produce red blood cells when more are needed. They also secrete a hormone that regulates blood pressure and keeps it in a normal range.   Q: What organs secrete hormones that help maintain homeostasis?   A:\\nThe answer is:', 'chosen': 'The kidneys are the organs that secrete hormones to help maintain homeostasis. They produce a hormone that stimulates bone marrow to produce red blood cells when needed, and they also secrete a hormone that regulates blood pressure, keeping it within a normal range.', 'rejected': ' Certainly! Based on the provided paragraph, the organs that secrete hormones to help maintain homeostasis are the kidneys. The kidneys produce two hormones that help regulate various physiological processes in the body:\\n\\n1. Erythropoietin (EPO): This hormone stimulates the bone marrow to produce red blood cells when there is a decrease in the number of red blood cells in the body. This helps to maintain normal red blood cell levels and prevent anemia.\\n2. Renin: This hormone regulates blood pressure by controlling the amount of fluid in the body and the diameter of blood vessels. When blood pressure is low, the kidneys produce more renin, which causes the blood vessels to constrict and retain more fluid. This helps to increase blood pressure back to normal levels.\\n\\nSo, to summarize, the organs that secrete hormones to help maintain homeostasis are the kidneys, specifically the nephrons within the kidneys. These hormones play a crucial role in regulating various physiological processes, such as red blood cell production and blood pressure, to maintain homeostasis in the body.'}\n",
      "the length of the train dataset is: 12859\n"
     ]
    }
   ],
   "source": [
    "'''data set'''\n",
    "\n",
    "print(train_dataset.dataset[10])\n",
    "print(\"the length of the train dataset is:\",len(train_dataset.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169654af",
   "metadata": {},
   "source": [
    "### 设置 Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc78d7",
   "metadata": {},
   "source": [
    "### DPO loss\n",
    "$$\n",
    "\\mathcal{L}_{DPO}(\\pi_{\\theta};\\pi_{ref})=-\\mathbb{E}_{(x,y_w,y_l)\\sim D}\\left[\\log \\sigma(\\beta\\log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)}-\\beta\\log \\frac{\\pi_{\\theta}(y_l | x)}{\\pi_{ref}(y_l | x)})\\right]\n",
    "$$\n",
    "采用整个序列预测 token 的对数概率之和，**采用**：\n",
    "$$\n",
    "\\log \\pi_{\\theta}(y_w | x) = \\sum\\limits_{i=k+1}^N\\log P_{\\theta}(y_i|x_{t<i})\n",
    "$$\n",
    "其中 k 是不需要计算 loss 的 prompt 输入 token 个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eda8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(logp_theta_w, logp_ref_w, logp_theta_l, logp_ref_l, beta):\n",
    "    log_ratio_w = logp_theta_w - logp_ref_w\n",
    "    log_ratio_l = logp_theta_l - logp_ref_l\n",
    "    \n",
    "    # 限制差异范围（防止数值不稳定）\n",
    "    log_ratio_w = torch.clamp(log_ratio_w, -10, 10)\n",
    "    log_ratio_l = torch.clamp(log_ratio_l, -10, 10)\n",
    "    \n",
    "    diff = beta * (log_ratio_w - log_ratio_l)\n",
    "    diff = torch.clamp(diff, -20, 20)  # 进一步限制\n",
    "    \n",
    "    loss = -F.logsigmoid(diff).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c6258",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "累积梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ae5fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 375/375 [06:27<00:00,  1.03s/it, loss=0.8016, avg_loss=0.5957]\n"
     ]
    }
   ],
   "source": [
    "def get_log_probs(model, input_ids, attention_mask, labels, is_ref_model=False):\n",
    "    # 如果是参考模型，强制禁用梯度\n",
    "    if is_ref_model:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    else:\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    selected_log_probs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "    mask = (labels != -100)\n",
    "    return (selected_log_probs * mask).sum(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_batches = len(train_dataloader)\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        \n",
    "\n",
    "        better_ids   = batch[\"input_ids_better\"].to(device)      # [B, Lp]\n",
    "        better_att  = batch[\"attention_mask_better\"].to(device)\n",
    "        worse_ids    = batch[\"input_ids_worse\"].to(device)      # [B, Lc]\n",
    "        worse_att   = batch[\"attention_mask_worse\"].to(device)\n",
    "        prompt_len   = batch[\"text_len\"].to(device)\n",
    "\n",
    "        # make labels\n",
    "        B, L = better_ids.shape\n",
    "        w_labels = better_ids.clone()\n",
    "        l_labels = worse_ids.clone()\n",
    "        token_pos = torch.arange(L, device=device).unsqueeze(0).expand(B, L)\n",
    "        w_labels[token_pos < prompt_len.unsqueeze(1)] = -100\n",
    "        \n",
    "        l_labels[token_pos < prompt_len.unsqueeze(1)] = -100\n",
    "\n",
    "        # 计算策略模型的 log-probs（保留梯度）\n",
    "        logp_w_theta = get_log_probs(model, better_ids, better_att, w_labels, is_ref_model=False)\n",
    "        logp_l_theta = get_log_probs(model, worse_ids, worse_att, l_labels, is_ref_model=False)\n",
    "\n",
    "        # 计算参考模型的 log-probs（禁用梯度）\n",
    "        logp_w_ref = get_log_probs(ref_model, better_ids, better_att, w_labels, is_ref_model=True)\n",
    "        logp_l_ref = get_log_probs(ref_model, worse_ids, worse_att, l_labels, is_ref_model=True)\n",
    "\n",
    "        # ===== DPO loss + backward =====\n",
    "\n",
    "        loss = dpo_loss(logp_w_theta, logp_w_ref, logp_l_theta, logp_l_ref, beta)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        if (batch_idx + 1) % grad_accum_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "\n",
    "\n",
    "        # 更新进度条显示\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{avg_loss:.4f}'\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b09daf",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c30abc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./dpo-FT/tokenizer_config.json',\n",
       " './dpo-FT/special_tokens_map.json',\n",
       " './dpo-FT/vocab.json',\n",
       " './dpo-FT/merges.txt',\n",
       " './dpo-FT/added_tokens.json',\n",
       " './dpo-FT/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859f817",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Output ===\n",
      "system\n",
      "You are an AI assistant that helps people find information.\n",
      "user\n",
      "Where would the best place to drive over the speed limit be?\n",
      "assistant\n",
      "I believe this is a very broad question and it's subjective because different people have different driving habits.\n",
      "The answer may vary depending on the type of vehicle being driven, the weather conditions, the time of day, etc. However, I think it's important to know what the most common places for speeding violations are found. This will help you determine where the safest area for driving at a high speed should be located. The location can be determined by using the formula: 50 miles per hour x\n",
      "\n",
      "=== Reference Model Output ===\n",
      "system\n",
      "You are an AI assistant that helps people find information.\n",
      "user\n",
      "Where would the best place to drive over the speed limit be?\n",
      "assistant\n",
      "The best place to drive over the speed limit will depend on various factors, including the specific location and circumstances of the driver. However, generally speaking:\n",
      "\n",
      "1. **Highway**: On highways, it's often safer to drive at a higher speed limit because there is less traffic and more room for maneuvering.\n",
      "\n",
      "2. **Urban Areas**: In urban areas with limited space, driving at high speeds can lead to accidents due to reduced visibility and increased risk of collisions.\n",
      "\n",
      "3. **School Zones**: In\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_msg = \"You are an AI assistant that helps people find information.\"\n",
    "user_msg   = \"Where would the best place to drive over the speed limit be?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_msg},\n",
    "    {\"role\": \"user\", \"content\": user_msg}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=100,do_sample=True)\n",
    "    ref_output_ids = ref_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "ref_output_text = tokenizer.decode(ref_output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=== Model Output ===\")\n",
    "print(output_text)\n",
    "print(\"\\n=== Reference Model Output ===\")\n",
    "print(ref_output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fcfd3-a41e-43b7-adf4-f1efea2c4e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (align-anything + Ascend)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
